{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bottle Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mductran/Bottle-Classifier/blob/master/Bottle_Classifier.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "UbDqz2B6p27i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "9a9d2438-b9ac-42c8-8442-203862f0c5ec"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nlhbX0eQp5tl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "from tensorflow.contrib.layers import flatten\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from random import randint\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9N4jDrlQqQ-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "905caa25-8b3a-42a1-ab83-07bca1064640"
      },
      "cell_type": "code",
      "source": [
        "img_dir = \"/content/gdrive/My Drive/STU.AI/camera/training_data_2nd/\"\n",
        "data = pd.read_csv(img_dir+'label.csv')\n",
        "\n",
        "X_data = []\n",
        "Y_data = []\n",
        "\n",
        "image_size = (28,28)\n",
        "data = shuffle(data)\n",
        "\n",
        "for i in range(len(data)):\n",
        "    filename = data.iloc[i,0]\n",
        "    label = data.iloc[i,2]\n",
        "        \n",
        "    img = plt.imread(img_dir+filename)\n",
        "    img = cv2.resize(img, image_size, interpolation=cv2.INTER_CUBIC)\n",
        "    X_data.append(img)\n",
        "    Y_data.append(label)\n",
        "    \n",
        "X_data = np.array(X_data)\n",
        "Y_data = np.array(Y_data)\n",
        "n_classes = int(max(Y_data))+1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GJWCyn4n-C4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rgb2gray = [0.299, 0.587, 0.114]\n",
        "# Convert images to grayscale\n",
        "X_gray = np.dot(X_data[...,:3], rgb2gray)\n",
        "\n",
        "# Normalize images\n",
        "X_gray = (X_gray/255-0.5)*2\n",
        "X_gray = X_gray.reshape(*X_gray.shape, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQLw6FB7-FO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dataAug(X_train, y_train):\n",
        "    datagen = ImageDataGenerator(rotation_range=10,zoom_range=0.10)\n",
        "    for x_batch, y_batch in datagen.flow(X_train, y_train, batch_size=X_train.shape[0], shuffle=False):\n",
        "        X_train_aug = x_batch.astype('uint8')\n",
        "        y_train_aug = y_batch\n",
        "        break\n",
        "\n",
        "    X_train = np.concatenate([X_train, X_train_aug])\n",
        "    y_train = np.concatenate([y_train,y_train_aug])\n",
        "    return X_train,y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vk7qASH6q1BX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8530a286-e002-4885-8d57-78b2d4b6f6b3"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def LeNet(x):    \n",
        "    # Hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    \n",
        "    # TODO: Layer 1: Convolutional. Input = 28x28x1. Output = 28x28x6.\n",
        "    W1 = tf.Variable(tf.truncated_normal(shape=(1, 1, 1, 6), mean = mu, stddev = sigma))\n",
        "    conv1 = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='VALID')\n",
        "    b1 = tf.Variable(tf.zeros(6))\n",
        "    conv1 = tf.nn.bias_add(conv1, b1)\n",
        "    print(\"layer 1 shape:\",conv1.get_shape())\n",
        "\n",
        "    # TODO: Activation.\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "    \n",
        "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
        "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "    \n",
        "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
        "    W2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
        "    conv2 = tf.nn.conv2d(conv1, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "    b2 = tf.Variable(tf.zeros(16))\n",
        "    conv2 = tf.nn.bias_add(conv2, b2)\n",
        "    print(\"layer 2 shape:\",conv2.get_shape())\n",
        "                    \n",
        "    # TODO: Activation.\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
        "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
        "    conv2 = flatten(conv2)\n",
        "    \n",
        "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "    W3 = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
        "    b3 = tf.Variable(tf.zeros(120))    \n",
        "    full1 = tf.add(tf.matmul(conv2, W3), b3)\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    full1 = tf.nn.relu(full1)\n",
        "    \n",
        "    # Dropout\n",
        "    # full1 = tf.nn.dropout(full1, keep_prob)\n",
        "\n",
        "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "    W4 = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
        "    b4 = tf.Variable(tf.zeros(84)) \n",
        "    full2 = tf.add(tf.matmul(full1, W4), b4)\n",
        "    \n",
        "    # TODO: Activation.\n",
        "    full2 = tf.nn.relu(full2)\n",
        "    \n",
        "    # Dropout\n",
        "    # full2 = tf.nn.dropout(full2, keep_prob)\n",
        "\n",
        "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
        "    W5 = tf.Variable(tf.truncated_normal(shape=(84, 14), mean = mu, stddev = sigma))\n",
        "    b5 = tf.Variable(tf.zeros(14)) \n",
        "    logits = tf.add(tf.matmul(full2, W5), b5)\n",
        "    \n",
        "    return logits\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_-eVljiL6yWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "778fc434-fa61-49f6-8714-4c8b0481d0ef"
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
        "y = tf.placeholder(tf.int32, (None))\n",
        "one_hot_y = tf.one_hot(y, n_classes)\n",
        "\n",
        "learning_rate = 0.001\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "logits = LeNet(x)\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    sess = tf.get_default_session()\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
        "        total_accuracy += (accuracy*len(batch_x))\n",
        "    return total_accuracy/num_examples,logits"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer 1 shape: (?, 28, 28, 6)\n",
            "layer 2 shape: (?, 10, 10, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YEX5dX8u-Mnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1837
        },
        "outputId": "0902d442-432b-49e2-ebfc-c0c0ab8a1c09"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    X_train,X_valid,y_train,y_valid = train_test_split(X_gray, Y_data, test_size = 0.3, random_state=50)\n",
        "           \n",
        "    X_train, y_train = dataAug(X_train, y_train)\n",
        "\n",
        "    num_examples = len(X_train)\n",
        "    \n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "        validation_accuracy,_ = evaluate(X_valid, y_valid)\n",
        "        print(\"EPOCH {} ...\".format(i+1))\n",
        "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
        "        \n",
        "    saver.save(sess, './LeNet')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1 ...\n",
            "Validation Accuracy = 0.154\n",
            "EPOCH 2 ...\n",
            "Validation Accuracy = 0.231\n",
            "EPOCH 3 ...\n",
            "Validation Accuracy = 0.355\n",
            "EPOCH 4 ...\n",
            "Validation Accuracy = 0.476\n",
            "EPOCH 5 ...\n",
            "Validation Accuracy = 0.516\n",
            "EPOCH 6 ...\n",
            "Validation Accuracy = 0.579\n",
            "EPOCH 7 ...\n",
            "Validation Accuracy = 0.648\n",
            "EPOCH 8 ...\n",
            "Validation Accuracy = 0.689\n",
            "EPOCH 9 ...\n",
            "Validation Accuracy = 0.722\n",
            "EPOCH 10 ...\n",
            "Validation Accuracy = 0.791\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 0.788\n",
            "EPOCH 12 ...\n",
            "Validation Accuracy = 0.853\n",
            "EPOCH 13 ...\n",
            "Validation Accuracy = 0.806\n",
            "EPOCH 14 ...\n",
            "Validation Accuracy = 0.839\n",
            "EPOCH 15 ...\n",
            "Validation Accuracy = 0.879\n",
            "EPOCH 16 ...\n",
            "Validation Accuracy = 0.850\n",
            "EPOCH 17 ...\n",
            "Validation Accuracy = 0.883\n",
            "EPOCH 18 ...\n",
            "Validation Accuracy = 0.897\n",
            "EPOCH 19 ...\n",
            "Validation Accuracy = 0.883\n",
            "EPOCH 20 ...\n",
            "Validation Accuracy = 0.894\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 0.923\n",
            "EPOCH 22 ...\n",
            "Validation Accuracy = 0.912\n",
            "EPOCH 23 ...\n",
            "Validation Accuracy = 0.916\n",
            "EPOCH 24 ...\n",
            "Validation Accuracy = 0.930\n",
            "EPOCH 25 ...\n",
            "Validation Accuracy = 0.916\n",
            "EPOCH 26 ...\n",
            "Validation Accuracy = 0.930\n",
            "EPOCH 27 ...\n",
            "Validation Accuracy = 0.941\n",
            "EPOCH 28 ...\n",
            "Validation Accuracy = 0.934\n",
            "EPOCH 29 ...\n",
            "Validation Accuracy = 0.945\n",
            "EPOCH 30 ...\n",
            "Validation Accuracy = 0.930\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 0.930\n",
            "EPOCH 32 ...\n",
            "Validation Accuracy = 0.934\n",
            "EPOCH 33 ...\n",
            "Validation Accuracy = 0.949\n",
            "EPOCH 34 ...\n",
            "Validation Accuracy = 0.927\n",
            "EPOCH 35 ...\n",
            "Validation Accuracy = 0.938\n",
            "EPOCH 36 ...\n",
            "Validation Accuracy = 0.949\n",
            "EPOCH 37 ...\n",
            "Validation Accuracy = 0.930\n",
            "EPOCH 38 ...\n",
            "Validation Accuracy = 0.930\n",
            "EPOCH 39 ...\n",
            "Validation Accuracy = 0.941\n",
            "EPOCH 40 ...\n",
            "Validation Accuracy = 0.941\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 0.941\n",
            "EPOCH 42 ...\n",
            "Validation Accuracy = 0.945\n",
            "EPOCH 43 ...\n",
            "Validation Accuracy = 0.941\n",
            "EPOCH 44 ...\n",
            "Validation Accuracy = 0.945\n",
            "EPOCH 45 ...\n",
            "Validation Accuracy = 0.941\n",
            "EPOCH 46 ...\n",
            "Validation Accuracy = 0.945\n",
            "EPOCH 47 ...\n",
            "Validation Accuracy = 0.941\n",
            "EPOCH 48 ...\n",
            "Validation Accuracy = 0.952\n",
            "EPOCH 49 ...\n",
            "Validation Accuracy = 0.949\n",
            "EPOCH 50 ...\n",
            "Validation Accuracy = 0.916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yhwF3TQf-XSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}